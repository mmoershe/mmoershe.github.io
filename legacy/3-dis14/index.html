<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>DIS13</title>
<link rel="stylesheet" type="text/css" href="styles/styles.css">
</head>
<body>
<p><a name="0"></a></p>
<button class="returnbutton"><a href="#0">
&uarr;
</a></button>
<div class="nav">
<a href="#1">1. Data Warehouse / OLAP</a><br />
<a href="#2">2. Datenvorbereitung</a><br />
<a href="#3">3. Assoziationsanalyse</a><br />
<a href="#4">4. Klassifikationsanalyse</a><br />
<a href="#5">5. Clusteranalyse</a><br />
<a href="#6">6. Neuronale Netzwerke</a><br />
<a href="#7">7. Empfehlungssysteme</a><br />
<a href="#8">8. User-Item-Ansatz</a><br />
<a href="#9">9. Item-Item-Ansatz</a><br />
<a href="#10">10. unsortiert</a><br />
</div>
<p><a name='1'></a></p>
<h1>Data Warehouse / OLAP</h1>
<p>Prüfung: Selber Stern-Schema erstellen?<br />
Übung 3 könnte Klausur sein, oder man bekommt Faktentabelle und soll Dimensionstabellen erstellen etc...  </p>
<p>"Fakten" in der Faktentabelle sind Werte ohne weitere Dimensionstabelle<br />
Beispiele, Folie 26: Anzahl  </p>
<h2>Zwei Arten von Datenbank-Anwendungen:</h2>
<h3>1. OLTP / Online Transaction Processing</h3>
<p>arbeiten immer auf dem aktuellsten Stand<br />
Zugriff umfasst immer eine kleine Datenmenge <br />
- einfach Lese- und Schreibaufgaben <br />
Schnelle Antwortzeiten benötigt!  </p>
<p><code>Inhalt des Moduls "Datenbanksysteme"</code>  </p>
<p>Ungeeignet für Data Mining, weil wir wollen:<br />
- ...historische Daten (aus verschiedenen Zeiträumen, nicht nur aktuell)<br />
- ...große Datenmengen<br />
- ...wir wollen Daten aggregieren, also längere Antwortzeiten wären keine Problem  </p>
<h3>2. OLAP / Online Analytical Processing</h3>
<p><code>Fokus dieses Moduls</code><br />
Es geht nicht um das alltägliche Tagesgeschäft, sondern eher um <em>(größere)</em> strategische Entscheidungen --&gt; <strong>DECISION SUPPORT</strong> <br />
historisch Daten<br />
große Datenmengen -&gt; Lange Lesetransaktionen [in dem Kontext auch akzeptabel]<br />
Meist Integration, Konsolidierung und Aggregation der Daten<br />
Was muss man bei OLAP Datenbanken beim Entwurf beachten?  </p>
<hr />
<p>OLTP- und OLAP- Anwendungen nicht auf dem selben Datenbestand ausgeführt werden.<br />
System für OLAP-Anwendungen nimmt man auch Data Warehouse<br />
Von Zeit zu Zeit werden Daten aus dem OLTP System in das Warehouse übertragen, aber auch von Dateien wie Excel usw. [Dadurch historische Daten?]  </p>
<hr />
<h2>Defintion</h2>
<blockquote>
<p>A Data Warehouse is a <strong>subject-oriented</strong>, <strong>integrated</strong>, <strong>non-volatile</strong>, and <strong>time variant</strong> colletion of data to support management decisions.<br />
<em>[W.H. Inmon, 1996]</em></p>
</blockquote>
<h3>subject-oriented / Fachorientiert</h3>
<ul>
<li>spezifisches Anwendungsziel  </li>
<li>irrelevanten Daten weglassen</li>
</ul>
<h3>integrated / Integrierte Datenbasis</h3>
<ul>
<li>Daten aus unterschiedlichen Datenquellen werden integriert, also zusammengefasst  </li>
</ul>
<h3>non-volatile / Nicht-flüchtige Datenbasis</h3>
<ul>
<li>stabile, persistente Datenbasis</li>
<li>Daten im Data Warehouse werden nur im äußersten Notfall geändert. Data Warehouse wächst dadurch immer weiter! [Kapazitäten!]  </li>
</ul>
<h3>time variant / Historische Daten</h3>
<ul>
<li>Speicherung über längeren Zeitraum </li>
<li>Vergleich der Daten über die Zeit möglich</li>
</ul>
<h2>Datenmodellierung</h2>
<p><strong>Fakten</strong><br />
- betriebswirtschaftliche Kennzahlen <br />
  - <em>[Erlöse, Gewinne, Verluste, Umsätze, ...]</em></p>
<p><strong>Dimensionen</strong><br />
- Betrachtung dieser Kennzahlen aus unterschiedlichen Perspektiven <br />
  - <em>[zeitlich, regional, produkzbezogen, ...]</em>  </p>
<p><strong>Hierarchien, Konsolidierungsebenen</strong><br />
- Unterteilung der Auswertungsdimensionen möglich <br />
    - <em>[zeitlich: Jahr, Quartal, Monat; regional: Bundesländer, Bezirke, Städte/Gemeinden; ...]</em></p>
<h2>Data Cube</h2>
<p><strong>Theoretisches Konstrukt</strong> welches dem Data Warehouse zu Grunde liegt. <br />
Hochdimensionaler Würfel  <br />
Kanten: Dimensionen<br />
Zelle: eine oder mehrere Kennzahlen <br />
zwei Operationen durchführen:  </p>
<h3>Roll-Up</h3>
<p>In der Hierarchie eine Stufe nach Oben <br />
Datan werden verdichtet  </p>
<blockquote>
<p>Weniger Attribute in <strong>GROUP BY</strong> =&gt; stärkere Verdichtung =&gt; Roll-Up</p>
</blockquote>
<h3>Drill-Down</h3>
<p>In der Hierarchie eine Stufe nach Unten<br />
auf feinerer Ebene gehen   </p>
<blockquote>
<p>Mehr Attribute in <strong>GROUP BY</strong> =&gt;  weniger starke Verdichtung =&gt; Drill-Down</p>
</blockquote>
<hr />
<p>Direkt Data Cube erstellen [MOLAP] war nicht erfolgreich und hat sich nicht durchgesetzt. Transformation war damals aber nicht nötig.<br />
Durchgesetzt haben sich die ROLAP [relationale OLAPs]<br />
Vorteil: Verfügbarkeit<br />
Speicherung und Zugriff muss gut umgesetzt sein.  </p>
<h2>Snowflake Schema vs Stern Schema</h2>
<table>
<thead>
<tr>
<th>Snowflake Schema</th>
<th>Star Schema</th>
</tr>
</thead>
<tbody>
<tr>
<td>Normalisierung</td>
<td>Denormalisierung</td>
</tr>
<tr>
<td>Vermeidung von Redundanzen</td>
<td>Sehr Redundant</td>
</tr>
<tr>
<td>Memory Efficient</td>
<td>Schnelle Anfragebearbeitung bei einfachen Queries</td>
</tr>
<tr>
<td>Hohe Skalierbarkeit</td>
<td>Einfache Erstellung/Wartung</td>
</tr>
<tr>
<td>High Data Integrity (FK)</td>
<td>Eventuell Update-Anomalien</td>
</tr>
<tr>
<td>-&gt; more Flexible</td>
<td></td>
</tr>
<tr>
<td>Geeignet für komplexere Systeme</td>
<td></td>
</tr>
</tbody>
</table>
<h3>Snowflake Schema</h3>
<p>Eigene Tabelle für jede Klassifikationsstufe / Hierarchiestufe<br />
Eine Faktentabelle und theoretisch unendliche untergeordnete Klassifikationstabellen<br />
<strong>normalisiert</strong><br />
skalierbar<br />
unterliegt keinen Update-Anomalien (durch Normalisierung und Verknüpfung durch Keys lassen sich Daten ohne Probleme/Auswirkungen ändern, da nicht direkt diese Zellen referenziert werden.)  <br />
relativ aufwendiges Zusammenholen von Informationen (VIELE JOINS)<br />
viele Foreign Keys<br />
dadurch geringe Redundanzen --&gt; effiziente Speichernutzung  </p>
<h3>Stern Schema</h3>
<p>Eine Faktentabelle <br />
Für jede Dimension EINE Dimensionstabelle<br />
--&gt; viel Redundanz!!<br />
Anfragebearbeitung aber schneller!!  </p>
<p><strong>STERN JOIN:</strong><br />
- SELECT<br />
  - Kenngrößen (evtl. aggregiert)<br />
  - Ergebnisgranularität (Dimensionen)<br />
    - z.B. Zeit.Monat, Geographie.Stadt<br />
- FROM<br />
  - Faktentabelle<br />
  - Dimensionstabellen<br />
- WHERE<br />
  - Verbundbedingungen <br />
  - Restriktionen in Dimensionen <br />
    - z.B. Produkt.Produktgruppe = "Elektrogeraete", Zeit.Monat = "Januar 2000", ...</p>
<hr />
<h2>Optimierungs-Heuristiken</h2>
<h3>Materialisierung von Aggregaten</h3>
<p>Aggregation nicht immer neu berechnen, sondern häufig genutzte Aggregationen materialisieren </p>
<h3>CUBE-Operator</h3>
<p>Erweiterung des <strong>group by</strong>s  und aggregiert Daten über mehrere Dimensionnen hinweg.<br />
Query-Komplexität reduziert<br />
Aggregierung wird effizient INTERN gerechnet.<br />
- Faktentabelle wird beispielsweise nur einmal gelesen  </p>
<h2>ROW STORE vs COLUMN STORE</h2>
<p>ROW STORE [klassisch] stellt eine normale Faktentabelle mit Zeilen und Spalten da.<br />
relative neue Innovation ist die COLUMN STORE.<br />
Jede einzelne Spalte wird zu einer eigenen Tabelle welche mit einer ID versehen ist.<br />
<strong>Vorteil:</strong><br />
- Schnelle Anfragebearbeitung, da relativ wenige Daten bei einer Anfrage bewegt werden. <br />
  - Es muss nicht die ganze Tabelle mit vielen für die Anfrage irrelevanten Spalten gelesen werden, sondern wirklich genau nur die relevante Spalte.  </p>
<p><strong>Nachteil:</strong><br />
- Generierung von Redundanz  </p>
<h2>Übung 3</h2>
<p>Kunde könnte mehrere Verträge haben. Keine Informationen in die Faktentabelle, welche man irgendwo anders herbekommt, keine berechneten Werte. [bool(SMS) ist vielleicht etwas redundant, abewesend von Empfänger Nummer könnte auf SMS hinweisen.]<br />
Hier ist jetzt kein Fakt in der Faktentabelle. Die Kombination von Information stellt aber schon den Fakt da.<br />
Both "Beginn" und "Ende" greifen auf die selbe Dimensionstabelle zu!<br />
Die Dauer wäre dann <em>Ende - Beginn</em><br />
Vorwarnung: wenn man eine Faktentabelle bekommt, dann Faktentabelle auch so lassen!! Das vorgegebene ist Fix! <br />
Bei "Verträge" auch Beginn und Ende machen, weil Kunde kann Vertrag auch beenden oder ändern. Darüber stellt man fest welcher Vertrag/Tarif gerade gilt.<br />
Im Stern-Schema müsste man beispielsweise alle Tarif Infos in die Verträge Tabelle kommen, um nicht eine weitere Dimensionstabelle zu machen. In der Praxis würde man eine weitere Dimensionstabelle machen. Hier macht es inhaltlich Sinn eine weitere Dimensionstabelle für Tarif oder Addresse zu machen. Auch in der Klausur ist er gnädig. Bei der Zeit weitere Dimensionstabellen wären aber nicht in Ordnung.<br />
VertragsID und Kundennummer sind ja in der Faktentabelle verbunden, also die Kundennummer muss nicht nochmal in die Verträge Tabelle.<br />
Bei der Empfänger Nummer im Optimalfall auch die Vorwahl erfassen und weitere Sachen erfassen. [Gehört die Nummer zu einem Unternehmen etc.. ]  </p>
<h3>Faktentabelle / Verbindungen</h3>
<table>
<thead>
<tr>
<th>Kundennummer</th>
<th>Beginn</th>
<th>Ende</th>
<th>Vertragsid</th>
<th>Empfänger Telefonnummer</th>
<th>bool(SMS)</th>
</tr>
</thead>
<tbody>
<tr>
<td>6848</td>
<td>30.01.2000 15:32:43</td>
<td>30.01.2000 15:35:57</td>
<td>567</td>
<td>058305839</td>
<td>False</td>
</tr>
</tbody>
</table>
<h3>Kunden</h3>
<table>
<thead>
<tr>
<th>Kundennummer</th>
<th>Geburtsdatum</th>
<th>Vorname</th>
<th>Nachname</th>
<th>Straße</th>
<th>Hausnummer</th>
<th>PLZ</th>
<th>Ort</th>
<th>Bundesland</th>
<th>Land</th>
<th>Geschlecht</th>
<th>Beruf</th>
</tr>
</thead>
<tbody>
<tr>
<td>6848</td>
<td>23.02.1988</td>
<td>Peter</td>
<td>Hansson</td>
<td>Siemensstraße</td>
<td>445</td>
<td>4235</td>
<td>Hansdorf</td>
<td>Hessen</td>
<td>Deutschland</td>
<td>Männlich</td>
<td>Schlosser</td>
</tr>
</tbody>
</table>
<h3>Zeit</h3>
<table>
<thead>
<tr>
<th>Dat_Uhrzeit</th>
<th>Tag</th>
<th>Monat</th>
<th>Jahr</th>
<th>Quartal</th>
<th>KW</th>
<th>Wochentag</th>
<th>Saison</th>
<th>bool(Ferien?)</th>
</tr>
</thead>
<tbody>
<tr>
<td>30.01.2000 15:32:43</td>
<td>30</td>
<td>01</td>
<td>2000</td>
<td>1</td>
<td>4</td>
<td>Dienstag</td>
<td>Winter</td>
<td>True</td>
</tr>
<tr>
<td>30.01.2000 15:35:57</td>
<td>30</td>
<td>01</td>
<td>2000</td>
<td>1</td>
<td>4</td>
<td>Dienstag</td>
<td>Winter</td>
<td>True</td>
</tr>
</tbody>
</table>
<h3>Verträge</h3>
<table>
<thead>
<tr>
<th>Vertragsid</th>
<th>TarifID</th>
<th>Telefonnummer</th>
</tr>
</thead>
<tbody>
<tr>
<td>567</td>
<td>7</td>
<td>05830583</td>
</tr>
</tbody>
</table>
<h3>Tarif</h3>
<table>
<thead>
<tr>
<th>TarifID</th>
<th>Mindestpreis</th>
<th>Preis_SMS</th>
<th>Grundgebühr</th>
</tr>
</thead>
<tbody>
<tr>
<td>7</td>
<td>9000€</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><a name='2'></a></p>
<h1>Datenvorbereitung</h1>
<p>Datenvorbereitung ist zwischen Selection und Data Mining und beschreibt Preprocessing und Transformation  </p>
<p><strong>Datensatz</strong> --&gt; <code>Ein Datensatz beschreibt einen geordneten Vektor von Ausprägungen, die ein Objekt (z.B. Kunde, Nutzer, Produkt) für eine fixe Menge von Variablen (Attribute, Eigenschaften, Merkmale) besitzt.</code><br />
<strong>Datenset</strong> --&gt; <code>Eine Menge von Datensätzen, die die gleiche Variablenstruktur besitzen, werden als Datenset bezeichnet.</code><br />
Data Cleaning, Data Integration, Data Transformation, Data Reduction</p>
<h2>Data Cleaning</h2>
<p>Daten können inkonsistent, unvollständig, verrauscht sein.</p>
<p><strong>Ursachen:</strong><br />
- Problem bei Eingabe, Übertragung oder Erfassung<br />
- Diskrepanz bei Namenskonvention <br />
- Duplizierte Datensätze<br />
  - Daten mehrfacht erhoben</p>
<p><strong>Wirkung:</strong><br />
Unvollständige, fehlende, widersprüchliche oder irrelevante Daten.  </p>
<p>Datenbereinigung: <br />
Ergänzen, Verrauschungen glätten, Korrigieren, entfernen  </p>
<h3>Missing Values</h3>
<p>Varianten<br />
1. Datensätze mit fehlenden Datensätzen werden nicht berücksichtigt<br />
2. Fehlende Werte eienr Variable durch Mittelwert oder Median ersetzen<br />
  - Eher für Einzelfälle<br />
3. wahrscheinlichsten Wert zum Auffüllen des fehlenden Wertes bestimmten <br />
  - Fehlenden Wert prognostizieren <br />
  - Hoher Aufwand <br />
4. Variable ersetzen durch künstliche binäre Variable ["value_exists_yn"]<br />
  - Informationsverlust <br />
  - Häufigste Lösung</p>
<h3>Verrauschte Daten glätten</h3>
<p>Einfluss von extremen Werten reduzieren und zufällige Datenschwankungen ausgleichen.  </p>
<h4>Binning</h4>
<p>array: list = [4, 8, 15, 21, 21, 24, 25, 28, 34]</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Bin1</td>
<td>4</td>
<td>8</td>
<td>15</td>
</tr>
<tr>
<td>Bin2</td>
<td>21</td>
<td>21</td>
<td>24</td>
</tr>
<tr>
<td>Bin3</td>
<td>25</td>
<td>28</td>
<td>34</td>
</tr>
</tbody>
</table>
<p>means</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Bin1</td>
<td>9</td>
<td>9</td>
<td>9</td>
</tr>
<tr>
<td>Bin2</td>
<td>22</td>
<td>22</td>
<td>22</td>
</tr>
<tr>
<td>Bin3</td>
<td>29</td>
<td>29</td>
<td>29</td>
</tr>
</tbody>
</table>
<p>boundaries <br />
--&gt; Min und Max von jedem Behälter finden und restliche Werte mit nächstgelegenen Extremwert ersetzen. </p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Bin1</td>
<td>4</td>
<td>4</td>
<td>15</td>
</tr>
<tr>
<td>Bin2</td>
<td>21</td>
<td>21</td>
<td>24</td>
</tr>
<tr>
<td>Bin3</td>
<td>25</td>
<td>25</td>
<td>34</td>
</tr>
</tbody>
</table>
<p>means (oder median) üblich, boundaries eher selten und für Verrauschungen innerhalb der Werte.  </p>
<h4>Clustering</h4>
<p>Durch Clustering Aureißer finden und diese eventuell eliminieren</p>
<h4>Regression</h4>
<p>Daten an eine Funktion anpassen <br />
starke Manipulation der Daten<br />
vorsichtig sein  </p>
<h2>Data Integration</h2>
<p><strong>Kombination von Daten aus mehreren Quellen in einem kohärenten Datenspeicher</strong>  </p>
<h3>Typische Aufgaben</h3>
<p>Schema Integration, die selbe Information kann in mehreren Datenbanken verschiedene Namen haben [Kundennummer, Kndnnmr, customernr...]<br />
Semantische Homogenität!<br />
Datenwertkonflikte beheben, unterschiedliche Darstellungen oder Skalen von Informationen, beispielsweise Datumformat, float to int, ...<br />
Redundante Variablen entfernen! [z.Bsp. mit Korrelationsanalyse]  </p>
<h2>Data Transformation</h2>
<h3>Normalisierung</h3>
<p>Variablen haben häufig andere Ausprägungen der Spannweiten<br />
--&gt; nervig für Data Mining<br />
Einflüsse müssen normiert werden.<br />
Zwei Normalisierungstypen: <br />
Min-max-Normalisierung und Z-Normalisierung<br />
Z-Normalisierung ist preferred<br />
Min-Max hat Probleme mit Ausreißer, die <br />
Normalisierung muss unbedingt dokumentiert werden. Es muss genau so wiederholbar sein um Fehler zu korrigieren oder Daten erneut zu analysieren  </p>
<p><strong>Min-Max-Normalisierung</strong> <br />
v' = ((v - min) / (max - min)) * (newmax - newmin) + newmin<br />
wenn newmin, newmax = [0, 1], dann reicht:<br />
v' = (v - min) / (max - min)  </p>
<p><strong>Z-Normalisierung</strong><br />
 v’ = (v - Mittelwert) / Standardabweichung<br />
Nach Z-Normalisation gilt: Mittelwert=0 und Standardabweichung=1</p>
<h3>Diskretisierung</h3>
<p>Wenn Variablen mit unerschiedlichen Skalierungen<br />
skalierungen: nominal (kategorial), ordinal, metrischen (kontinuierlich)  </p>
<p>metrische Variable in eine ordinale oder nominale variable konvertieren.<br />
Unterteilung des Bereichs des Attributs in Intervalle<br />
--&gt; gewisse Informationsverlust  </p>
<p>Ähnlichkeit zum Binning <br />
1. Paritionierung mit gleicher Breite (Abstand)<br />
Aufteilung in gleich gro0e Intervalle (einheitliches Gitter)<br />
Breite des Intervals: W = (hoechsterWert - niedrigsterWert)/N<br />
Problem: Ausreißer haben großen Einfluss</p>
<ol>
<li>Partitionierung mit gleicher Tiefe (Häufigkeit)<br />
Bessere Datenskalierung, hilft gegen Ausreißern  </li>
</ol>
<h3>Dichotomisierung</h3>
<p>Konvertierung von nominaler Variable zu metrischer Variable<br />
keine richtige metrische Variable, sondern Dummy-Variable<br />
Umwandlung von Dichotom zu 0 und 1<br />
macht aber sinn wenn man Wahrscheinlichkeit oder so summiert oder sowas.<br />
Redundanz beachten und entfernen!  </p>
<p><em>Beispiel:</em><br />
Man hat die Variable 'Outlook' mit der Domain = [overcast, rain, sunny]<br />
und erstellt Dummy Variablen:<br />
- Outlook_overcast<br />
- Outlook_rain<br />
- Outlook_sunny<br />
mit den Domains = [0, 1]<br />
Einer dieser Variablen kann komplett wegfallen, da man auf einer der Variablen mit Hilfe der anderen schließen kann<br />
(Outlook_sunny löschen weil redundant. Outlook_sunny = 1 if Outlook_overcast == 0 and Outlook_rain == 0)  </p>
<h2>Data Reduction</h2>
<p>Leistungsfähigkeit verbessern.<br />
Repräsentative Reduktion<br />
--&gt; Datensätze entfernen, welche durch andere Datensätze repräsentiert werden.<br />
Entfernung sollte zufällig geschehen<br />
Reduziertes und unreduziertes Ergebnis sollte ähnlich sein!<br />
Vielleicht Daten verdichten (von Quartal auf Year)  </p>
<h2>Rapid Miner</h2>
<p>nichts links bei input, soll ja alles automatisiert sein!!!!!!!<br />
Row number ist nicht wie ID durch operator (Generate ID)<br />
Rollen sieht man nicht rofl, aber für einen selber für später<br />
"ori" gibt Daten so aus wie sie in den Operator reingekomme sind.<br />
nicht originale Datenbank verändern.  </p>
<p><a name='3'></a></p>
<h1>Assoziationsanalyse</h1>
<p>Ursprung: Supermarkt: Welche Produkte werden gleichzeitig gekauft? (Point-of-Sale-Daten), geht aber mittlerweile darüber hinaus.<br />
Es geht darum, Verbindungen zwischen Objekten zu finden, welche in Transaktion vorkommen.<br />
Man möchte Regeln finden, welche das Vorhandensein einer Menge von Items mit einer anderen Menge von Items verknüpft.<br />
Wir sind an folgdenden Regeln interessiert, welche...<br />
- nicht-trivial (vielleicht sogar unterwartet), <br />
- praktisch umsetzbar, <br />
- erklärbar sind.</p>
<p>Wir haben eine Menge <strong><em>M</em></strong> von Transaktionen gegeben<br />
|TransactionID|Items|<br />
|---|---|<br />
|100|A, B, C|<br />
|200|A, B|<br />
|300|A, D|<br />
|400|B, E, F|</p>
<p><strong><em>|M|</em></strong> ist die Anzahl der Transaktionen in <strong><em>M</em></strong><br />
<strong><em>|M|</em></strong> = 4<br />
<strong><em>I</em></strong> ist die Menge der unterschiedlichen Item in <strong><em>M</em></strong><br />
<strong><em>I</em></strong> = {A, B, C, D, E, F}<br />
Form the Regel: X -&gt; Y (Aus Menge X folgt Menge Y)<br />
wobei A ∩ B = ∅ [X und Y sind disjunkt]  </p>
<h2>Assoziationsregeln</h2>
<p>Aufbau: Prämisse {body} -&gt; Schlussfolgerung {head}<br />
(IM RPM: Premise --&gt; Conclusion)<br />
beides werden als Menge von Items dargestellt.<br />
die zwei wichtigen Maßzahl zur Bewertung dieser Regeln sind Support und Konfidenz</p>
<h3>Support-Anzahl(Itemset): σ() [Sigma]</h3>
<p>Anzahl der Transaktionen in <strong><em>M</em></strong>, in den das Itemset vorkommt.<br />
σ({A, B}) = 2  </p>
<h3>Support(Itemsets)</h3>
<p>Relativer Anteil der Transaktionen <strong><em>M</em></strong>, in denen das Itemset enthalten ist.<br />
sup(Itemset) = σ(itemset) / <strong><em>|M|</em></strong><br />
= 2/5</p>
<h3>minsup / frequent Itemsets</h3>
<p>Ein künstlicher erdachter Grenzwerkt für den Support eines Itemsets.<br />
Itemsets mit sup(Itemset) &gt;= minsup gelten als <strong><em>frequent Itemsets</em></strong></p>
<h3>Support(X -&gt; Y)</h3>
<ul>
<li>bewertet Unterstützung der Regel</li>
<li>Anteil der Transaktionen welche beide Mengen beinhalten von allen Transaktionen<br />
Support der Vereinigung von den Mengen X (Prämisse) und Y (conclusion) <br />
sup(x -&gt; Y) = sup(X u Y)</li>
</ul>
<h3>Konfidenz(X -&gt; Y)</h3>
<ul>
<li>bewertet Verlässlichkeit der Regel </li>
<li>Anteil Transkationen, bei denen beide Mengen vorkommen von allen Transkationen, bei denen die Prämissen-Menge vorkommt.  </li>
<li>Bei der Konfidenz ist die RICHTUNG wichtig, beim Support nicht. </li>
<li>deswegen werden sie ein wenig unterschiedlich aufgeschrieben. <ul>
<li>conf(A --&gt; B)</li>
<li>supp(A, B)<br />
conf(X -&gt; Y) = σ(X u Y)/ σ(X) = sup(X -&gt; Y) / sup(x)</li>
</ul>
</li>
</ul>
<p><em>Beispielhafte, konkrete Idee:<br />
Bei einer Assoziationsregel mit hoher Konfidenz sollte man nicht Produkte aus beiden Mengen rabattieren!<br />
[Da das jeweils andere Produkt ja sowieso mitgekauft wird.]</em>  </p>
<blockquote>
<p>Für <strong><em>n</em></strong> Items gibt es 2^<strong><em>n</em></strong> Itemsets!  </p>
</blockquote>
<h3>Lift</h3>
<p>Eine hohe Konfidenz ist nur aussagekräftig, wenn der Support gering ist.<br />
--&gt; Wenn eine Produkt sowieso sehr häufig gekauft wird [-&gt; hoher Support], dann ist logischerweise eine hohe Konfidenz das Ergebnis, jedoch nicht für die Assoziation oder Kombination der Produkte aussagekräftig.<br />
Für dieses Problem gibt es die Maßzahl <strong>Lift</strong>, welcher die Konfidenz und den Support in Verhältnis setzt.<br />
hoch = gut, höher als 1  </p>
<p>lift(X -&gt; Y) = conf(X -&gt; y) / sup(y)<br />
lift(X -&gt; Y) = sup(X U Y) / sup(X)sup(Y)</p>
<h2>Apriroi-Ansatz</h2>
<p>(statt Brute-Force-Ansatz)<br />
Idee:<br />
Wenn ein Itemset häufig ist, sind auch die Teilmengen des Itemsets häufig!<br />
--&gt; wenn {A, B} häufig gekauft wird, dann wird das Itemset {A} oder {B} mindestens auch so häufig gekauft<br />
Warum? <code>Anti-Monotonie-Eigenschaft des Supports</code><br />
<strong>auch andersherum:</strong><br />
Wenn ein Itemset <em>nicht</em> häufig ist, gilt das auch für jede Obermenge dieses Itemsets!!  </p>
<p>Mit dem Apriroi-Algorithmus schaut man Schritt für Schritt ob der minsupp gegeben ist und wirft Mengen und Untermengen raus, welche diese nicht erfüllen. Dann bildet und rechnet man Regeln mit verbliebenden Mengen.  </p>
<h2>Bewertung</h2>
<ul>
<li>Sortiere die Regeln nach Support und Konfidenz absteigend.</li>
<li>Ein hoher Support gibt wieder, dass sich die Regel häufig<br />
anwenden lässt (Relevanz), während die Konfidenz die<br />
Verlässlichkeit der Regel widerspiegelt (Effizienz).</li>
<li>In der Praxis sind oft die Regeln mit hohem Support und<br />
hoher Konfidenz bereits bekannt. Deshalb sind meistens vor<br />
allem die Regeln im Mittelfeld interessant.</li>
<li>Zusätzlich kann der Lift berechnet werden. Je höher der Lift<br />
um so stärker wird das Auftreten der Schlussfolgerung durch<br />
die Prämissen erhöht.</li>
</ul>
<h2>RapidMiner</h2>
<p><img alt="image" src="raw/assoziationsregeln_rapidminer.PNG" /></p>
<h3>FP_Growth</h3>
<p><strong><em>frequent itemsets</em></strong> finden, also Itemsets, mit einem gewissen Mindestsupport.<br />
<em>positive value</em>: Was gilt als True?<br />
<em>min support</em>: minsup<br />
<em>find number of itemsets</em>: true or false<br />
-&gt; <em>min number of datasets</em>: int</p>
<h3>Create Association Rules</h3>
<p>Erstellt aus frequent itemsets Assoziationsregeln und filtert alle Regeln nach einem Kriterium.<br />
<em>criterion</em>: Kriterium nachdem gefiltert werden soll, meist confidence [confidence, lift, conviction...]<br />
<em>min confidence / min criterion</em>: minimum was vom ausgewählten Kriterium in der Regel gegeben sein muss.  </p>
<p><a name='4'></a></p>
<h1>Klassifikationsanalyse</h1>
<p>Objekte in vorgegeben Klassen einordnen, Objekte sind durch Variablen bestimmt.<br />
Zuerst Model erstellen auf Basis von Objekten, wessen Klassenzuordnung wir kennen, dann auf unbekannte anwenden.  </p>
<h2>Allgemein</h2>
<p><strong>Klassifizierungsleistung/Classification accuracy/Trefferquote, Prognosequote</strong>: Anteil der Objekte die korrekt klassifiziert werden. <br />
(sollte zwischen Validierungs- und Trainingsdate relativ identisch sein.)  </p>
<p>Es ist eventuell sogar erwünscht, nur eine Accuracy von 90% bei den Trainingsdaten zu erhalten, um eine bessere Generalisierungsleistung zu erzielen.<br />
also eine 100% konsistentes Model bei den Trainingsdaten ist eventuell <strong><em>overfitted</em></strong><br />
OCCAM'S RAZOR<br />
- einfache Regeln und Variablen häufig besser<br />
  - Einfaches Model spart Daten und ist leichter zu vermitteln.<br />
  - Bessere Generalisierungsleistung  </p>
<p>Model soll Daten Trainingsdaten nicht auswendig lernen, sondern natürlich general einsetzbar sein! (Generalisierungsleistung)  </p>
<p>Effizienzaspekte: <br />
1. Trainingsdauer<br />
  - Effizienz des Algorithus um das Modell zu trainieren <br />
2. Anwendungsdauer<br />
  - Effizienz der Anwendung des Models </p>
<h2>Dreistufiger-Prozess</h2>
<h3>Model Konstruktion (Training / Learning)</h3>
<p><strong>Zielvariable / target</strong><br />
- Variable im Trainings-Datensatz, der sich die vordefinierte Klasse entnehmen lässt.</p>
<p><strong>Klassenbezeichnung / class labels</strong><br />
- Ausprägung der Zielvariable</p>
<p><strong>Trainingsdaten / training set</strong><br />
- Menge des Trainings-Datensatzes, welches zum Trainierungs oder Lernen verwendet wird. </p>
<p><em>Modell kann in verschiedenen Formen repräsentiert werden, einige Beispiele:</em><br />
- Decision Trees, Regelwerk, Wahrscheinlichkeiten, Neuronale Netzwerke, If Statements...</p>
<h3>Model Validierung (Klassifizierungsleistung)</h3>
<p><strong>Validierungsdaten / test set</strong><br />
- Trainingsdatensätze, welche nicht für's Modeltrainings verwendet wurde. </p>
<p>Trefferquote auf Trainingsdaten und Validierungsdaten bestimmen.<br />
<strong>Der Unterschied sollte kleiner als 10% sein, sonst nicht ausreichende Generalisierungsleistung.</strong>  </p>
<blockquote>
<p>Üblicherweise werden 70-80% der Trainings-Datensätze zum Trainieren und 20-30% für die Validierungsdaten verwendet.<br />
-&gt; Man möchte logischerweise eine gute Basis für das Training schaffen, jedoch gleichzeitig ausreichend validieren!</p>
</blockquote>
<p>Ergebnis der Model Validierung ist eine <strong>Klassifizierungsmatrix/confusion matrix)</strong></p>
<p>Zwei Klassen: T und F</p>
<p><img alt="confusion_matrix" src="raw/confusion_matrix.PNG" /></p>
<p>Gesamttrefferquote: (20 + 18) / 38  = 87%<br />
Trefferquote Klasse T: 18 / 20<br />
Trefferquote Klasse F: 15 / 18  </p>
<h3>Model Anwendung</h3>
<p>Das Modell wird verwendet um für unklassifizierte Objekte die Klasse zu prognostizieren. </p>
<hr />
<h2>Decision Trees</h2>
<p>Statistische Zusammenhänge != Kausalität  </p>
<p>Widerspruchsfrei!  </p>
<p>Achtung vor Overfitting, sorgt für schlechte Generalisierungsleistung.  </p>
<p><strong>Wurzelknoten</strong><br />
- Oberster / Erster Knoten</p>
<p><strong>Innere Knoten</strong><br />
- Repräsentiert Ausprägung einer Variable</p>
<p><strong>Äste / Kanten</strong><br />
- Verbindung zwischen Knoten<br />
- Repräsentiert eines Tests  </p>
<p><strong>Blattknoten</strong><br />
- Letzter Knoten, von welchen keine weiteren Äste abgehen<br />
- Repräsentiert Klassen-Bezeichnung</p>
<p><strong>Pfad</strong><br />
- Weg vom Wurzelknoten bis zu einem Blattknoten</p>
<p>Wenn Variablen metrisch, dann mit Intervallen arbeiten!<br />
[if temperature &gt; 80...]  </p>
<p>Jeder Pfad in einem Entscheidungsbaum repräsentiert eine Regel.<br />
--&gt; jeder Entscheidungsbaum lässt sich in Regeln konvertieren  </p>
<h3>Konstruktieren</h3>
<h4>1. Baumkonstruktion</h4>
<p>Rekursiver Prozess aus drei Schritten.  </p>
<h4>2. Baumbeschneidung (Pruning)</h4>
<p>Zur Verbesserung der Generalisierungsleistung / Auswendiglernen reduzieren<br />
Äste zu Knoten entfernen, welche nur durch wenige Datensätze gestützt sind.  </p>
<h3>Variablen-Auswahl im decision Tree</h3>
<p>Es gibt einige Maßzahlen zur optimalen nächsten Variablenauswahl.<br />
Alle haben Vor- und Nachteile, am besten alle ausprobieren und bestes Ergebnis benutzen.<br />
Zufällige Auswahl hat auch schon gute Ergebnisse  </p>
<h4>Information Gain</h4>
<p>Information Gain hilft, um Variable zu finden, welche ...<br />
Wiederholen lol  </p>
<h4>Gain Ratio</h4>
<p>"Information Gain" bevorzugt Variablen mit groß Anzahl von Ausprägungen  <br />
C4.5-Algorithmus<br />
tendiert zu unbalancierten Bäumen (extremer Kontrast bei Länge der Pfäden)  </p>
<h4>Gini Index</h4>
<p>pass </p>
<h3>Overfitting and Pruning</h3>
<p><strong>Overfitting</strong><br />
- Baum lernt lediglich Trainingsdaten auswendig<br />
  - aufgrund von vielen Blattknoten und relativ wenigen Trainingsdatensätzen..<br />
  - Spezialisierung<br />
- Kleine (zufällige) Unterschiede in den Trainingsdaten werden zu eigenen Regeln, wobei diese generalisiert nicht anwendbar sind.  </p>
<blockquote>
<p>Ein überangepasster Decision Tree hat möglicherweise Regeln entwickelt, die speziell auf die Nuancen der Trainingsdaten zugeschnitten sind, ohne die zugrunde liegenden Muster zu erfassen. Das führt dazu, dass der Baum auf neuen Daten schlecht generalisiert. </p>
</blockquote>
<p><strong>Prepruning</strong><br />
- Baum vorher beschneiden. <br />
- Knoten entfernen, welche nur durch wenige Datensätzen gestützt werden.<br />
  - Was sind "wenige" Datensätze?<br />
  - <br />
<strong>Postpruning</strong><br />
- Nachträglich beschneiden <br />
- Bäume verschieden beschneiden und nachher besten durch Tests herausfinden.<br />
Beispiel: Jeder Blattknoten muss durch &gt;10 Datensätze gestützt sein.  </p>
<h2>Rapid Miner</h2>
<p><img alt="RapidMiner Decision Tree" src="raw/decision_tree.PNG" /></p>
<h3>Split Data</h3>
<p>Aufteilung der Datensätze in Trainings- und Testmenge. Verhältnis: ~7-3</p>
<h3>Discretize Freq / Apply Discreticise Model</h3>
<p>Diskretisierung, da ID3 Algorithmus nur mit nominal skalierten Variabel umgehen kann<br />
subset attribute auswählen, man kann auswahl invertieren<br />
<em>range name type</em>: <strong>interval</strong> / long / short<br />
<em>number of bins</em>: Anzahl der Bins bei Intervallbildung  </p>
<h3>ID3</h3>
<p>erstellt unpruned Decision Tree, Orientierung an ID3 Algorithmus, braucht nominal skalierte Daten<br />
<em>criterion</em>: information gain / gain ratio / gini index / accuracy [Variablen Auswahl]<br />
<em>minimal size for split</em>: <strong>2</strong><br />
<em>minimal leaf size</em>: <strong>1</strong></p>
<h3>Multiply</h3>
<p>Mulitpliziert hier das trainierte Model, sodass wir es auf Trainings- UND Testmenge anwenden können!</p>
<h3>Apply Model</h3>
<p>Wendet ein model auf ein Datenset an.<br />
Hier wendet es einmal den durch die Trainingsmenge trainierten DecisionTree auf die Trainingsmenge (oben) und auf die Testmenge an.   </p>
<h3>Performance (Classification)</h3>
<p>Nimmt LabelledData von einer Klassifikationsanalyse und bewertet diese, in dem es eine <strong><em>confusion matrix</em></strong> erstellt.<br />
es lässt sich ein main criterion auswählen, <em>accuracy</em> macht hier am meisten Sinn  </p>
<p><a name='5'></a></p>
<h1>Clusteranalyse</h1>
<p>Objekte in Gruppen einteilen.<br />
[Im Unterschied zur Klassifikation sind diese Gruppen jedoch vorher nicht bekannt!]<br />
--&gt; Gruppen werden aus Daten raus generiert.<br />
Es kann auch Objekte geben, welche sich keinem Cluster zuordnen lassen [Ausreißer]<br />
Ein Cluster bildet eine Gruppe von Objekten welche sich <em>ähnlich</em> sind.<br />
Objekte unterschiedlicher Cluster sind sich <em>unähnlich</em>  </p>
<h2>Segmentierungsansatz (naiv / einfach)</h2>
<p>Eine Variable wählen, für jede Ausprägung eine Gruppe bildeln<br />
Nächste Variable und Untergruppen bilden<br />
Ein Cluster würde beispielsweise alle "unter 20 Jährigen, weiblich, einkommen über 1000€, gute Bildung" abbilden<br />
Exponentionell viele Gruppen mit jeweils relativ wenigen Objekten<br />
Diskretisierung nötig -&gt; Informationsverlust<br />
Schlechte Ergebnisse, dieser Ansatz hat viele Probleme </p>
<h2>Anforderungen an ein ordentliches Clustersystem</h2>
<ul>
<li>Variablenausprägungen welche <em>ähnlich</em> sind sollen zusammengefasst werden, nicht nur identische.  </li>
<li>Robuste Gruppenbildung </li>
<li>keine fundamentalen Änderungen bei missing values, Austauschungen oder Reihenfolgenänderungen</li>
<li>Überschaubare Anzahl an Clustern </li>
<li>~ 5 bis 7 Cluster </li>
</ul>
<h2>Änhlichkeit bzw. Unähnlichkeit</h2>
<p>Aus Ähnlichkeitsmaß lässt sich das Distanzmaß errechnen:<br />
<strong>1 - Ähnlichkeitsmaß = Distanzmaß</strong><br />
Andersherum ist dies nur möglich, wenn man das Maximum kennt.  </p>
<p>Manhattenn-Distanz<br />
Euklidische-Distanz<br />
Cosinusähnlichkeit  </p>
<p>Cosinusähnlichkeit benutzen, wenn 0 keine inhaltliche Relevanz hat</p>
<p>Bei Manhattan und Eklid ist eventuell problematisch, dass die "0" gleich keine Eingabe steht.  </p>
<p>Tanimoto Maß ist ein guter Kompromiss.  </p>
<h3>Ähnlichkeitsmaß</h3>
<ul>
<li>Wert höher je größer die Änhlichkeit</li>
<li>Maximaler Wert beim Vergleich von identischen Objekten</li>
<li>Maximaler Wert kann jedoch auch mit nicht identischen Objekten erreicht werden. </li>
<li>Intervall [0,1]</li>
</ul>
<h3>Unähnlichkeit -&gt; Distanzmaß</h3>
<ul>
<li>Wert höher je größer die Distanz</li>
<li>Minimaler Wert beim Vergleich von identischen Objekten </li>
<li>Maß nach oben unbeschränkt!</li>
</ul>
<h2>Partitionierende Verfahren / k-means-Algorithmus</h2>
<ul>
<li>Schnell und für große Datenmengen geeignet. </li>
<li>Iterativ</li>
<li>Objekte können bis zur letzten Iteration das Cluster ändern  </li>
</ul>
<p>Clusterzahl k muss vorgeben werden [Problem]<br />
  - inhaltlich begründet<br />
  - basierend auf Ergebnisse anderer Verfahren <br />
  - Vorgaben <br />
  - Zufall und Variation <br />
  - Silhouetten-Koeffizient<br />
Anfangspartition festlegen und Objekte einsortieren <br />
- Zufall und Variation<br />
- basierend auf Ergebnisse anderer Verfahren <br />
Änhlichkeit von jedem Objekt zu jedem Clauster berechnen und zur größten Ähnlichkeit verschieben </p>
<p>iterative Erkennung eines Centroiden und Berechnung der Ähnlichkeit zu eben jenem Centroiden<br />
Platzierung der anfänglichen Centroiden ist relevant fürs Ergebnis!  </p>
<h2>Agglomeratives / Hierarchisches Verfahren</h2>
<ul>
<li>Für kleine Datenmengen </li>
<li>Findet gut Ausreißer</li>
<li>agglomerative, Objekte werden zu Clustern zusammengeführt</li>
<li>Jedes Objekt fängt als eigenes Cluster an  </li>
<li>Paarweise Änhlichkeiten berechnen und das Cluster mit größter Ähnlichkeit zusammenführen [Anzahl Cluster =- 1]  </li>
<li>Wiederholung</li>
</ul>
<p>Ähnlichkeit zwischen Clustern ist nicht gleich Ähnlichkeit zwischen Objekten:  </p>
<h3>Ähnlichkeit zwischen Clustern</h3>
<ul>
<li>single-linkage </li>
<li>Ähnlichkeit zwischen den beiden Clustern ist die Ähnlichkeit zwischen den zwei nächstgelegenen Datenpunkten aus jeweils eines der beiden Clustern </li>
<li>Identifiziert Ausreißer</li>
<li>Complete-linkage </li>
<li>Ähnlichkeit zwischen zwei Clustern ist die Ähnlichkeit zwischen den beiden entferntesten Datenpunkten aus jeweils eines der beiden Clustern </li>
<li>Ausreißer sorgen für Probleme </li>
</ul>
<p>Häufig Kombination der Methoden.<br />
Erst single-linkage, dann Ausreißer entfernen und mit complete-linkage beenden.  </p>
<p>Im Dendogramm die optimale Clusterzahl finden (visuell) [erster großer Sprung]  <br />
Mit dem Silhouetten-Koeffizient  lässt sich auch die optimale Clusterzahl berechnen  </p>
<h2>Silhouetten-Koeffizient</h2>
<p>Die Silhouette von Objekt i S(i) drückt aus, wie sehr ein Objekt in das zugeordnete Cluster A passt.<br />
Ergebnis liegt zwischen 1 und -1<br />
Der <strong><em>Silhouetten-Koeffizient</em></strong> ist einfach das arithmetische Mittel aller Silhouetten Objekte in einem Cluster c oder im Gesamtdatensatz.<br />
Die optimale Clusteranzahl <em>k</em> lässt sich hiermit ermitteln.<br />
Das Clustering mit dem höchsten Silhouetten-Koeffizienten enthält das optimale k  </p>
<p>Interpretation-Faustregeln:<br />
s(i) &lt; 0.25 -&gt; schlecht<br />
0.25 &lt; s(i) &gt; 0.5 -&gt; mittelmäßig<br />
0.5 &lt; s(i) -&gt; gut  </p>
<<<<<<< Updated upstream
=======
<h2>Rapid Miner Clusteranalyse</h2>
<h3>K-MEANS</h3>
<p><img alt="clustering k-means" src="raw/cluster_k_means.PNG" /></p>
<h4>Shuffle</h4>
<p>k-means Ergebnisse sind teilweise von Reihenfolge der Datensätze abhänig.<br />
random seed Angabe möglich</p>
<h4>Clustering  [k-means]</h4>
<p>Der eigentliche k-Means-Algorithmus.<br />
<em>add cluster attribute</em>: [x] <br />
--&gt; erstellt Attribut <strong>cluster</strong> und trägt clusternumer ein.<br />
<em>k</em>: int<br />
<em>max runs</em>: int<br />
<em>determine good starts values</em>: [x]<br />
--&gt; uses k-means++<br />
<em>measure type</em>: NumericalMeasure, NominalMeasure, MixedMeasure<br />
bei measure type = NumericalMeasure:<br />
<em>numerical measure</em>: EuclideanDistance, Cosinus-Ähnlichkeit (cosineSimilarity), manhatten distance...<br />
<em>random seed</em>  </p>
<h4>Performance((Average) Silhouette) [Plugin]</h4>
<p>berechnet Silhoutte für jedes Objekt, avg Silhouette für jeden Cluster UND avg Silhouette für den ganzen Gesamtdatensatz  </p>
<h4>Correlation Matrix // Remove Corerlated Attributes</h4>
<p>Wichtig für eine Clusterlösung ist, dass die beiden Attribute nicht korrelieren. Mit dem Operator <strong><em>Correlation Matrix</em></strong> stellt die Correlation zwischen allen Attributen dar. Hinzufügend kann der Operator <strong><em>Remove Correlated Attributes</em></strong> automatisch korrelierende Attribute rausfiltern.<br />
<strong><em>Üblicher Grenzwert: 0.8</em></strong> (gilt mit und ohne Vorzeichen)</p>
<h3>AGGLOMERATIV / HIERARCHISCH</h3>
<p><img alt="agglomerative_rapid_miner" src="raw/cluster_agglomerativ.PNG" /></p>
<h4>Clustering (Agglomerative Clustering)</h4>
<h4>multiply</h4>
<p>Cluster Model verdoppeln, um es gleichzeitig weiterzubenutzen UND um es einfach auszugeben um sich das Dendogram anzuzeigen.<br />
Dendogramm kann Auskunft darüber geben, wieviele Cluster optimal sind.  </p>
<h4>Flatten Clustering</h4>
<p>Agglomerative Clustering läuft immer weiter, flatten clustering nimmt einen Stand von mittendrin.<br />
<em>number of clusters</em></p>
<h4>Data to similiarity</h4>
<p>berechnet Ähnlichkeit von ExampleSet, nötig um Silhouette auszugeben UND anderes Ähnlichkeitsmaß zu benutzen.<br />
<em>measure type</em>:<br />
<em>x measure</em>:  </p>
>>>>>>> Stashed changes
<p><a name='6'></a></p>
<h1>Neuronale Netzwerke</h1>
<p>wenn kein Spaltenname in Excel vergeben, macht der Rapid Miner eine Durchnummerierung<br />
blackbox, kein Regelsystem, jedoch leistungsfähig af<br />
Neuronen, weights, Abbild des Gehirns  </p>
<p>fp-growth minimum support bestimmte prozentzahl</p>
<p>Assoziatioanalyse = Ähnlichkeit von Worten innerhalb eines Dokumentes<br />
cluster = ähnlichkeit zwischen Dokumenten  [read csv, rename, select attributes, set role, hierarchische clustering für Ausreißer (cosinusÄhnlichkeit), flatten Clustering, data to similarity, performance]  </p>
<p>perceptron ist ein Neuronales netzwerk ohne hidden layer  </p>
<p><a name='7'></a></p>
<h1>Empfehlungssysteme</h1>
<p>Assoziationsanalyse nicht perfekt, einfach zu viele Regln und kleine Confidence, man könnte aber einfach die höhste Confidence nehmen<br />
(kann man für Empfehlungssysteme nutzen, aber nicht optimal.)<br />
Klassifikationsanalyse, was zeichnet Personen aus, welche ein bestimmtes Video schauen? (sozio-demographisch, andere geschaute Videos), auch aufwendig, da man für jedes Video eine Klassifikationsanalyse durchführen müsste; kommt eher nur für wichtige Dinge in Einsatz, wenn sich der Aufwand lohnt.<br />
Clusteranalyse auch möglich, bestimmte Videos sind ähnlich, Videos im gleichen Cluster vorschlagen.<br />
ODER Personen clustern<br />
aus dieser Idee kommt das, was sich durchgesetzt hat:<br />
Collaborative Filtering<br />
"was haben Personen, welche mir ähnlich sind, ein bestimmtes Produkt bewertet?"<br />
--&gt; Deren hochbewertete Produkte/Videos werden mir wieder empfohlen.<br />
Wann sind andere mir genau "ähnlich"? Was heißt hier "ähnlich"?<br />
Zwei Ansätze: <br />
<a name='8'></a></p>
<h1>User-Item-Ansatz</h1>
<p>Wie haben Nutzer ein bestimmtes Item bewertet?<br />
und wie habe ICH dieses bestimmte item bewertet?<br />
Haben wir eine ähnliches Bewertungsgeschichte?<br />
Wenn wir ähnliche Bewertungen haben, die andere Person aber einen weitere Film, welche ich nicht kenne, positiv bewertet, wird er mir auch gefallen.<br />
Wenn wir ähnliche Bewertungen haben, die andere Person aber einen weitere Film, welche ich nicht kenne, positiv bewertet, wird er mir auch gefallen.<br />
Nachteil: <br />
Wenn neue Nutzer/Items dazukommen, muss alles neu berechnet werden, dafür aber gute Ergebnisse (Hoher Aufwand)<br />
Vorschläge müssen ja auch schnell generiert werden.  </p>
<p><a name='9'></a></p>
<h1>Item-Item-Ansatz</h1>
<p>KLAUSUR: In der Lage sein, beide Ansätze erklären, nicht konkret ausrechnen.<br />
Nicht Nutzer werden verglichen, sondern Items.<br />
Welche Objekte haben von den selben Nutzern ähnliche Bewertungen bekommen.<br />
Ählnichkeit wird über Items hergestellt.<br />
Ich schlage einem Nutzer mit einer Vorliebe für ein bestimmtes Item ein ähnlich Item mit gleichen Bewertungen vor.<br />
Viel schneller.<br />
"Wie werden Objekte bewertet"?  Es geht NUR ums Rating!<br />
Metadaten, wie Länge des Films, Schauspieler, Genre, etc..., sind erstmal egal. <br />
Es werden fast keine Daten erhoben.<br />
Einfaches Prinzip<br />
nicht viele Bewertungen nötig.<br />
Individuelle Bewertung ins Verhältnis von der allgemeinen Bewertung setzen.  </p>
<p><a name='10'></a></p>
<h1>unsortiert</h1>
<p>Klausur zwei stunden <br />
kein SQL  </p>
<h2>Blöcke</h2>
<p>Klausur sind drei Teile aus vier möglichen Blöcken<br />
Wenn er uns eine Faktentabelle gibt, muss diese so bleiben, Bezeichnung von Attributen NICHT ändern.  </p>
<ol>
<li>Allgemeines zum Data Mining(Theorie) / Data Warehouse/OLAP  <ul>
<li>~ die ersten beiden Vorlesungen  </li>
</ul>
</li>
<li>Algorithmen auf einem konkreten Beispiel anwenden (Praxis), <em>Assoziationsanalyse, Entscheidungsbaumverfahren, Clusteranalyse (hierarchische und k-means)</em> (hat nichts mit dem RapidMiner zu tun)  <ul>
<li>Entscheidungsbaum muss zu den Daten passen. Beispielregel angeben, anzahl Regeln angeben.  </li>
<li>clusteranalyse, neighest neighbour oder das andere vorgegeben</li>
<li>Eventuell Buntstifte und Geodreieck für die Distanz mitnehmen lol  </li>
<li>bei jedem Zeichenschritt bisschen schreiben was man gemacht hat  </li>
</ul>
</li>
<li>RapidMiner, man bekommt Ergebnis und muss es interpretieren, oder ein Parameterfenster und einzelne Parameter erklären, nur was in Tutorials ist!  <ul>
<li>Erst beschreiben, was man sieht. Dabei ruhig kurz, aber präzis. Schrittweise pro Operator.  </li>
<li>wenn offensichtlich ist, welcher Algorithmus es ist, auch hinzufügen. FP-Growth und Create Association Rules ist OFFENSICHTLICH zur Erzeugung von Assoziationsregeln in der Assoziationsanalyse.  </li>
</ul>
</li>
<li>TextMining, Empfehlungssysteme; konkretes Anwendungsbeispiel   </li>
<li>SEHR FREI, details ausdenken und diese interpretieren. Wieder ohne Bezug zum RapidMiner, grobes Vorgehen  </li>
</ol>
<p>garbage in - garbage out </p>
<p>Heuristiken, kein perfektes Ergebnis im Unterschied zur Statistik </p>
<p>Neuronale Netzwerk sind leistungsfähig af und in dem Aspekt allen Verfahren überlegen, der Vorteil der andereren System ist die Transparenz und Nachvollziehbarkeit. Ethische Fragen!?!?!??"!?§"!?§?"!§?"!§?$§I$%  </p>
<p>Korrelation != Kausalität  </p>
<p>In der Prüfung keine SQL Befehle  </p>
<p>In der Klausur bekommt man Bilder vom Rapid Miner und man muss die Operationen erklären<br />
auch einzelne Paramter erklären<br />
und Ergebnisse erklären<br />
alles in den Tutorials an Prozessen  </p>
<p>in rapid miner heißt NN nicht unbedingt Neuronales Netz, sondern neighest Neighbour!!!!!!!!!!!!!!!!!  </p>
<script>var codeElements = document.querySelectorAll('code'); codeElements.forEach(function(codeElement) { var brElement = document.createElement('br'); codeElement.parentNode.insertBefore(brElement, codeElement);});</script>
</body>